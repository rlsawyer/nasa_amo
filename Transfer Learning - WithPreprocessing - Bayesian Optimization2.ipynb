{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlee/venvp3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.applications import VGG16, InceptionV3, ResNet50, VGG19, Xception\n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_flag = 1\n",
    "\n",
    "if model_flag == 1:\n",
    "    vgg_conv = VGG16(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(224, 224, 3))\n",
    "    vgg_conv.summary()\n",
    "elif model_flag == 2:\n",
    "    inc_conv = InceptionV3(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))\n",
    "elif model_flag == 3:\n",
    "    resnet_conv = ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))\n",
    "elif model_flag == 4:\n",
    "    vgg19_conv = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))\n",
    "elif model_flag == 5:\n",
    "    xcep_conv = Xception(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(299,299,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1829 images belonging to 2 classes.\n",
      "Obtaining training data\n",
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/home/rlee/Documents/Pneumothorax/model/mmode/train'\n",
    "validation_dir = '/home/rlee/Documents/Pneumothorax/model/mmode/val'\n",
    "\n",
    "nTrain = 1829\n",
    "nVal = 457\n",
    "\n",
    "if model_flag == 5:\n",
    "    datagen = ImageDataGenerator(rescale=1./255,data_format=\"channels_last\")\n",
    "    target_sz = 299\n",
    "else:\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    target_sz = 224\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "if model_flag == 1:\n",
    "    d1 = 7\n",
    "    d2 = 7\n",
    "    d3 = 512\n",
    "    model_name = 'VGG16'\n",
    "elif model_flag == 2:\n",
    "    d1 = 5\n",
    "    d2 = 5\n",
    "    d3 = 2048\n",
    "    model_name = 'InceptionV3'\n",
    "elif model_flag == 3:\n",
    "    d1 = 1\n",
    "    d2 = 1\n",
    "    d3 = 2048\n",
    "    model_name = 'Resnet50'\n",
    "elif model_flag == 4:\n",
    "    d1 = 7\n",
    "    d2 = 7\n",
    "    d3 = 512\n",
    "    model_name = 'VGG19'\n",
    "elif model_flag == 5:\n",
    "    d1 = 10\n",
    "    d2 = 10\n",
    "    d3 = 2048\n",
    "    model_name = 'Xception'\n",
    "\n",
    "train_features = np.zeros(shape=(nTrain, d1, d2, d3))\n",
    "\n",
    "train_labels = np.zeros(shape=(nTrain,2))\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(target_sz, target_sz),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "print('Obtaining training data')\n",
    "i = 0\n",
    "for inputs_batch, labels_batch in train_generator:\n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "    if model_flag == 1:\n",
    "        features_batch = vgg_conv.predict(inputs_batch)\n",
    "    elif model_flag == 2:\n",
    "        features_batch = inc_conv.predict(inputs_batch)\n",
    "    elif model_flag == 3:\n",
    "        features_batch = resnet_conv.predict(inputs_batch)\n",
    "    elif model_flag == 4:\n",
    "        features_batch = vgg19_conv.predict(inputs_batch)        \n",
    "    elif model_flag == 5:\n",
    "        features_batch = xcep_conv.predict(inputs_batch) \n",
    "    \n",
    "    train_features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "    train_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "    i += 1\n",
    "    if i * batch_size >= nTrain:\n",
    "        break\n",
    "\n",
    "\n",
    "train_features = np.reshape(train_features, (nTrain, d1 * d2 * d3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 457 images belonging to 2 classes.\n",
      "Obtaining validation data\n"
     ]
    }
   ],
   "source": [
    "validation_features = np.zeros(shape=(nVal, d1, d2, d3))\n",
    "\n",
    "validation_labels = np.zeros(shape=(nVal,2))\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(target_sz, target_sz),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "\n",
    "print('Obtaining validation data')\n",
    "i = 0\n",
    "for inputs_batch, labels_batch in validation_generator:\n",
    "    if model_flag == 1:\n",
    "        features_batch = vgg_conv.predict(inputs_batch)\n",
    "    elif model_flag == 2:\n",
    "        features_batch = inc_conv.predict(inputs_batch)\n",
    "    elif model_flag == 3:\n",
    "        features_batch = resnet_conv.predict(inputs_batch)    \n",
    "    elif model_flag == 4:\n",
    "        features_batch = vgg19_conv.predict(inputs_batch)        \n",
    "    elif model_flag == 5:\n",
    "        features_batch = xcep_conv.predict(inputs_batch) \n",
    "    \n",
    "    validation_features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "    validation_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "    i += 1\n",
    "    if i * batch_size >= nVal:\n",
    "        break\n",
    "\n",
    "validation_features = np.reshape(validation_features, (nVal, d1 * d2 * d3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers, initializers, regularizers, callbacks\n",
    "\n",
    "activ = 'sigmoid'\n",
    "loss = 'categorical_crossentropy'\n",
    "optim = 1\n",
    "init = initializers.RandomNormal()\n",
    "bnorm_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model():\n",
    "    def __init__(self, drpout, init, activ, bnorm_flag, lrt, loss, optim, decay_rt, decay_steps, \n",
    "                 batch_size, n_epochs, train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "        self.drpout = drpout\n",
    "        self.init = init\n",
    "        self.activ = activ\n",
    "        self.bnorm_flag = bnorm_flag\n",
    "        self.lrt = lrt\n",
    "        self.loss = loss\n",
    "        self.optim = optim\n",
    "        self.decay_rt = decay_rt\n",
    "        self.decay_steps = np.max([np.round(decay_steps*n_epochs), 1])\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.val_X = val_X\n",
    "        self.val_y = val_y        \n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "        self.model = self.set_up_model()\n",
    "        \n",
    "    def set_up_model(self):\n",
    "        \n",
    "        model = models.Sequential()\n",
    "        \n",
    "        '''\n",
    "        if self.init == 0:\n",
    "            init = initializers.RandomNormal()\n",
    "        else:\n",
    "            init = initializers.glorot_normal()\n",
    "        \n",
    "        activ = 'relu'\n",
    "        if self.activ == 1:\n",
    "            activ = 'relu'\n",
    "        elif self.activ == 2:\n",
    "            activ = 'tanh'\n",
    "        elif self.activ == 3:\n",
    "            activ = 'sigmoid'\n",
    "            \n",
    "        if self.loss == 1:\n",
    "            loss = 'categorical_crossentropy'\n",
    "        elif self.loss == 2:\n",
    "            loss = 'mean_squared_error'\n",
    "        elif self.loss == 3:\n",
    "            loss = 'kullback_leibler_divergence'\n",
    "        elif self.loss == 4:\n",
    "            loss = 'categorical_hinge'\n",
    "        '''\n",
    "        \n",
    "        model.add(layers.Dense(d3, activation=self.activ, input_dim=d1 * d2 * d3, kernel_initializer=self.init))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "        if self.bnorm_flag:\n",
    "            model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(self.drpout))\n",
    "        model.add(layers.Dense(2, activation='softmax', kernel_initializer=self.init))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "        \n",
    "        optim = optimizers.RMSprop(lr=self.lrt)\n",
    "        if self.optim == 1:\n",
    "            optim = optimizers.RMSprop(lr=self.lrt)\n",
    "        elif self.optim == 2:\n",
    "            optim = optimizers.Adam(lr=self.lrt)\n",
    "        elif self.optim == 3:\n",
    "            optim = optimizers.SGD(lr=self.lrt)\n",
    "            \n",
    "        model.compile(optimizer=optim, loss=loss, metrics=['acc'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def step_decay(self, epoch):\n",
    "            steps = self.decay_steps#np.floor(n_epochs/3)\n",
    "            lrate = self.lrt * (self.decay_rt)**np.floor(epoch/steps)\n",
    "            return lrate\n",
    "        \n",
    "    def train_model(self):\n",
    "        \n",
    "        earlystop = EarlyStopping(patience=10)\n",
    "        checkpointer = callbacks.ModelCheckpoint(filepath='/home/rlee/weights.hdf5', verbose=1, save_best_only=True)\n",
    "        lrate=keras.callbacks.LearningRateScheduler(self.step_decay,verbose=1)\n",
    "        callbacks_list = [earlystop, lrate, checkpointer]\n",
    "        history = self.model.fit(self.train_X,\n",
    "                self.train_y,\n",
    "                epochs=self.n_epochs,\n",
    "                callbacks=callbacks_list,\n",
    "                batch_size=self.batch_size,\n",
    "                validation_data=(self.val_X,self.val_y))\n",
    "        #dscr_phrase = 'rmsprop' + str(lrt) + 'decay_' + str(decay_rt) + '_' + loss + 'glorot_normal_init'# + 'dropout_' + str(drpout)\n",
    "        #model.save(model_name + '_mmode_nopreprocessing_' + dscr_phrase + '.h5')\n",
    "        self.model = keras.models.load_model('/home/rlee/weights.hdf5')\n",
    "        \n",
    "    def model_evaluate(self):\n",
    "        self.train_model()\n",
    "        \n",
    "        evaluation = self.model.evaluate(self.test_X, self.test_y, batch_size=self.batch_size, verbose=0)\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_my_model(drpout, init, activ, bnorm_flag, lrt, loss, optim, decay_rt, decay_steps, \n",
    "                 batch_size, n_epochs, train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "    \n",
    "    model = my_model(drpout=drpout, init=init, activ=activ, bnorm_flag=bnorm_flag, lrt=lrt, loss=loss, \n",
    "                     optim=optim, decay_rt=decay_rt, decay_steps=decay_steps, batch_size=batch_size, \n",
    "                     n_epochs=n_epochs, train_X=train_X, train_y=train_y, val_X=val_X, val_y=val_y, \n",
    "                     test_X=test_X, test_y=test_y)\n",
    "    model_evaluation = model.model_evaluate()\n",
    "    return model_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds for hyper-parameters in model\n",
    "# the bounds dict should be in order of continuous type and then discrete type\n",
    "bounds = [{'name': 'drpout', 'type': 'continuous',  'domain': (0.0, 0.8)},\n",
    "          {'name': 'lrt',          'type': 'continuous',  'domain': (1e-6, 1e-3)},\n",
    "          {'name': 'decay_rt',          'type': 'continuous',  'domain': (1e-6, 1.0)},\n",
    "          {'name': 'decay_steps',           'type': 'discrete',    'domain': (0.1, 0.2, 0.3, 0.5)},\n",
    "          {'name': 'batch_size',       'type': 'discrete',    'domain': (16, 32, 64, 128)},\n",
    "          {'name': 'n_epochs',           'type': 'discrete',    'domain': (20, 30, 50, 100)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to optimize mnist model\n",
    "def f(x):\n",
    "    print(x)\n",
    "    evaluation = run_my_model(drpout=float(x[:,0]), init=init, activ=activ, bnorm_flag=bnorm_flag, \n",
    "                              lrt=float(x[:,1]), loss=loss, optim=optim, decay_rt=float(x[:,2]), \n",
    "                                        decay_steps=int(x[:,3]), batch_size=int(x[:,4]), n_epochs=int(x[:,5]),\n",
    "                             train_X=train_features, train_y=train_labels, val_X=validation_features,\n",
    "                             val_y=validation_labels, test_X=validation_features, test_y=validation_labels)\n",
    "    print(\"LOSS:\\t{0} \\t ACCURACY:\\t{1}\".format(evaluation[0], evaluation[1]))\n",
    "    print(evaluation)\n",
    "    return evaluation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.01015912e-01 3.00535730e-04 1.88362094e-01 1.00000000e-01\n",
      "  6.40000000e+01 5.00000000e+01]]\n",
      "Train on 1829 samples, validate on 457 samples\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.00030053572971616656.\n",
      "1829/1829 [==============================] - 13s 7ms/step - loss: 0.2322 - acc: 0.9120 - val_loss: 0.1710 - val_acc: 0.9278\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17096, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.6609539464509264e-05.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.0744 - acc: 0.9727 - val_loss: 0.1417 - val_acc: 0.9387\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17096 to 0.14171, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1.0663091411494977e-05.\n",
      "1829/1829 [==============================] - 5s 3ms/step - loss: 0.0651 - acc: 0.9765 - val_loss: 0.0993 - val_acc: 0.9562\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14171 to 0.09932, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2.008522230094839e-06.\n",
      "1829/1829 [==============================] - 5s 3ms/step - loss: 0.0598 - acc: 0.9787 - val_loss: 0.0988 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09932 to 0.09878, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 3.783294537301123e-07.\n",
      "1829/1829 [==============================] - 5s 3ms/step - loss: 0.0594 - acc: 0.9803 - val_loss: 0.0987 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09878 to 0.09873, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 7.126292824399889e-08.\n",
      "1829/1829 [==============================] - 5s 2ms/step - loss: 0.0601 - acc: 0.9814 - val_loss: 0.0988 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 1.3423234410747466e-08.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.0667 - acc: 0.9781 - val_loss: 0.0990 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 2.52842854603085e-09.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.0638 - acc: 0.9765 - val_loss: 0.0990 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 4.76260096245141e-10.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.0587 - acc: 0.9792 - val_loss: 0.0990 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 8.97093491653149e-11.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.0584 - acc: 0.9814 - val_loss: 0.0990 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 1.689784088802189e-11.\n",
      "1829/1829 [==============================] - 3s 1ms/step - loss: 0.0635 - acc: 0.9792 - val_loss: 0.0990 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 3.182912698995525e-12.\n",
      "1829/1829 [==============================] - 3s 1ms/step - loss: 0.0603 - acc: 0.9776 - val_loss: 0.0991 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 5.995401019906832e-13.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.0610 - acc: 0.9792 - val_loss: 0.0991 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 1.1293062923417119e-13.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.0637 - acc: 0.9792 - val_loss: 0.0992 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 2.127184983436525e-14.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.0608 - acc: 0.9803 - val_loss: 0.0992 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "LOSS:\t0.09872852978693969 \t ACCURACY:\t0.9540481400437637\n",
      "[0.09872852978693969, 0.9540481400437637]\n",
      "[[7.81873103e-01 7.60192341e-04 1.74638637e-01 5.00000000e-01\n",
      "  6.40000000e+01 3.00000000e+01]]\n",
      "Train on 1829 samples, validate on 457 samples\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0007601923410082022.\n",
      "1829/1829 [==============================] - 11s 6ms/step - loss: 0.3334 - acc: 0.8688 - val_loss: 0.2622 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26216, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00013275895433082234.\n",
      "1829/1829 [==============================] - 3s 1ms/step - loss: 0.1555 - acc: 0.9404 - val_loss: 0.2158 - val_acc: 0.9147\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26216 to 0.21576, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 2.3184842840745225e-05.\n",
      "1829/1829 [==============================] - 3s 1ms/step - loss: 0.1199 - acc: 0.9546 - val_loss: 0.1587 - val_acc: 0.9256\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.21576 to 0.15865, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 4.048969353965877e-06.\n",
      "1829/1829 [==============================] - 3s 1ms/step - loss: 0.1175 - acc: 0.9557 - val_loss: 0.1352 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15865 to 0.13525, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 7.071064894407497e-07.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1148 - acc: 0.9502 - val_loss: 0.1256 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13525 to 0.12564, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 1.2348811356635295e-07.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1284 - acc: 0.9475 - val_loss: 0.1214 - val_acc: 0.9431\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12564 to 0.12135, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 2.1565795845314844e-08.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.1203 - acc: 0.9502 - val_loss: 0.1186 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12135 to 0.11863, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 3.7662211933612476e-09.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1334 - acc: 0.9492 - val_loss: 0.1174 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11863 to 0.11736, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 6.577277360438787e-10.\n",
      "1829/1829 [==============================] - 3s 1ms/step - loss: 0.1233 - acc: 0.9546 - val_loss: 0.1165 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11736 to 0.11651, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 1.1486467537381084e-10.\n",
      "1829/1829 [==============================] - 3s 1ms/step - loss: 0.1300 - acc: 0.9442 - val_loss: 0.1161 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11651 to 0.11609, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 2.0059810352669617e-11.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1145 - acc: 0.9584 - val_loss: 0.1158 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11609 to 0.11576, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 3.5032179395060347e-12.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.1170 - acc: 0.9508 - val_loss: 0.1155 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11576 to 0.11550, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.11797206250439e-13.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.1417 - acc: 0.9420 - val_loss: 0.1154 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11550 to 0.11542, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 1.0684343025162149e-13.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1134 - acc: 0.9546 - val_loss: 0.1152 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11542 to 0.11524, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 1.8658991037072778e-14.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1225 - acc: 0.9519 - val_loss: 0.1152 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11524 to 0.11521, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 3.2585807634744914e-15.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1194 - acc: 0.9557 - val_loss: 0.1152 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11521 to 0.11515, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 5.690741032561107e-16.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.1258 - acc: 0.9541 - val_loss: 0.1151 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11515 to 0.11505, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 9.93823257740721e-17.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1244 - acc: 0.9459 - val_loss: 0.1151 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.7355993920213134e-17.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.1148 - acc: 0.9546 - val_loss: 0.1150 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11505 to 0.11504, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 3.0310271229038128e-18.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.1208 - acc: 0.9568 - val_loss: 0.1150 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.11504 to 0.11498, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 5.293344456106924e-19.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1234 - acc: 0.9524 - val_loss: 0.1150 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.11498 to 0.11495, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 9.244224612597464e-20.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1157 - acc: 0.9535 - val_loss: 0.1149 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11495 to 0.11487, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.614398786943907e-20.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.1097 - acc: 0.9563 - val_loss: 0.1148 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.11487 to 0.11484, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2.819364038098203e-21.\n",
      "1829/1829 [==============================] - 3s 2ms/step - loss: 0.1273 - acc: 0.9508 - val_loss: 0.1150 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 4.923698929660798e-22.\n",
      "1829/1829 [==============================] - 5s 3ms/step - loss: 0.1233 - acc: 0.9497 - val_loss: 0.1149 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 8.598680703289325e-23.\n",
      "1829/1829 [==============================] - 5s 3ms/step - loss: 0.1190 - acc: 0.9546 - val_loss: 0.1149 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.5016618784653e-23.\n",
      "1829/1829 [==============================] - 5s 2ms/step - loss: 0.1207 - acc: 0.9530 - val_loss: 0.1149 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 2.622481837676928e-24.\n",
      "1829/1829 [==============================] - 5s 3ms/step - loss: 0.1135 - acc: 0.9492 - val_loss: 0.1148 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.11484 to 0.11481, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 4.579866538247664e-25.\n",
      "1829/1829 [==============================] - 4s 2ms/step - loss: 0.1328 - acc: 0.9486 - val_loss: 0.1147 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.11481 to 0.11468, saving model to /home/rlee/weights.hdf5\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 7.998216501183124e-26.\n",
      "1829/1829 [==============================] - 5s 3ms/step - loss: 0.1267 - acc: 0.9508 - val_loss: 0.1148 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "LOSS:\t0.11468241529241693 \t ACCURACY:\t0.9540481400437637\n",
      "[0.11468241529241693, 0.9540481400437637]\n",
      "[[4.33090440e-01 5.05786768e-04 5.28658931e-01 1.00000000e-01\n",
      "  1.60000000e+01 2.00000000e+01]]\n",
      "Train on 1829 samples, validate on 457 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "import GPy, GPyOpt\n",
    "\n",
    "opt_model = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds)\n",
    "opt_model.run_optimization(max_iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Parameters:\n",
      "\tdrpout:\t0.09117103101966963\n",
      "\tlrt:\t0.00021736975729206076\n",
      "\tdecay_rt:\t0.95286935869545\n",
      "\tdecay_steps:\t0.3\n",
      "\tbatch_size:\t64.0\n",
      "\tn_epochs:\t100.0\n",
      "\n",
      "optimized loss: [0.07863146]\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Optimized Parameters:\n",
    "\\t{0}:\\t{1}\n",
    "\\t{2}:\\t{3}\n",
    "\\t{4}:\\t{5}\n",
    "\\t{6}:\\t{7}\n",
    "\\t{8}:\\t{9}\n",
    "\\t{10}:\\t{11}\n",
    "\"\"\".format(bounds[0][\"name\"],opt_model.x_opt[0],\n",
    "           bounds[1][\"name\"],opt_model.x_opt[1],\n",
    "           bounds[2][\"name\"],opt_model.x_opt[2],\n",
    "           bounds[3][\"name\"],opt_model.x_opt[3],\n",
    "           bounds[4][\"name\"],opt_model.x_opt[4],\n",
    "           bounds[5][\"name\"],opt_model.x_opt[5],))\n",
    "print(\"optimized loss: {0}\".format(opt_model.fx_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
